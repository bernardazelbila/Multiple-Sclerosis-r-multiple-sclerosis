import os
from collections import Counter
import spacy

# ----------------- SETTINGS -----------------

BASE_DIR = "/Users/user/Desktop/Other Docs/Multiple Sclerosis/Data/cleaned"

INPUT_PATH = os.path.join(BASE_DIR, "r_MultipleSclerosis_corpus_cleaned.txt")

# Output: corpus without stopwords
OUTPUT_CORPUS_PATH = os.path.join(BASE_DIR, "r_MultipleSclerosis_corpus_cleaned_nostop.txt")

# Output: nodes and edges
NODES_OUT = os.path.join(BASE_DIR, "ms_emotion_nodes_nostop.csv")
EDGES_OUT = os.path.join(BASE_DIR, "ms_emotion_edges_nostop.csv")

# Window size for co-occurrence (± N words around each emotion)
WINDOW_SIZE = 4

# 16 anchor emotions
EMOTIONS = [
    "anger", "frustration", "guilt", "joy", "hope", "love", "pride", "relief",
    "dislike", "distress", "fear", "regret", "sadness", "contempt", "disgust", "shame"
]
EMOTION_SET = set(EMOTIONS)

# ----------------- LOAD RAW TEXT -----------------

if not os.path.exists(INPUT_PATH):
    raise FileNotFoundError(f"Corpus file not found: {INPUT_PATH}")

print(f"Reading corpus from: {INPUT_PATH}")
with open(INPUT_PATH, "r", encoding="utf-8", errors="ignore") as f:
    raw_text = f.read()

print(f"Length of raw text: {len(raw_text)} characters")

# ----------------- SET UP SPACY & REMOVE STOPWORDS -----------------

print("Initialising spaCy pipeline for tokenisation and stopword removal...")

# Load light English model (no parser/ner/tagger to save memory)
nlp = spacy.load("en_core_web_sm", disable=["parser", "ner", "tagger", "lemmatizer"])

# Allow large text if needed
nlp.max_length = max(len(raw_text) + 1000, 2_500_000)

doc = nlp(raw_text)

tokens_nostop = []

for token in doc:
    # Skip spaces and punctuation
    if token.is_space or token.is_punct:
        continue

    text_lower = token.text.lower()

    # Keep numbers? -> we skip them
    if token.like_num:
        continue

    # Remove stopwords EXCEPT if they are one of the 16 emotions
    if token.is_stop and text_lower not in EMOTION_SET:
        continue

    tokens_nostop.append(text_lower)

print(f"Total tokens (after stopword removal): {len(tokens_nostop)}")

# ----------------- SAVE CORPUS WITHOUT STOPWORDS -----------------

os.makedirs(os.path.dirname(OUTPUT_CORPUS_PATH), exist_ok=True)

with open(OUTPUT_CORPUS_PATH, "w", encoding="utf-8") as f:
    f.write(" ".join(tokens_nostop))

print(f"Corpus without stopwords written to: {OUTPUT_CORPUS_PATH}")

# ----------------- BUILD NODE & EDGE COUNTS (CENTERED ON EMOTIONS) -----------------

print("Building node and edge counts around the 16 emotion anchors...")

node_freq = Counter()     # frequency of each node
edge_weights = Counter()  # (label_a, label_b) -> weight (undirected)

n_tokens = len(tokens_nostop)

for i, tok in enumerate(tokens_nostop):
    # Only anchor on emotion tokens
    if tok not in EMOTION_SET:
        continue

    # Count the emotion node itself
    node_freq[tok] += 1

    start = max(0, i - WINDOW_SIZE)
    end = min(n_tokens, i + WINDOW_SIZE + 1)

    for j in range(start, end):
        if j == i:
            continue

        coll = tokens_nostop[j]

        # Skip empty items just in case
        if not coll:
            continue

        # Count the collocate node
        node_freq[coll] += 1

        # Build undirected edge between tok (emotion) and coll
        # If coll is also an emotion, this naturally creates emotion–emotion edges
        a, b = sorted((tok, coll))
        edge_weights[(a, b)] += 1

print(f"Unique nodes in emotion-centred windows: {len(node_freq)}")
print(f"Unique edges (emotion-centred co-occurrences): {len(edge_weights)}")

# ----------------- ASSIGN IDS TO NODES -----------------

# Sort nodes alphabetically for stable IDs
sorted_nodes = sorted(node_freq.items(), key=lambda x: x[0])
label_to_id = {label: idx for idx, (label, freq) in enumerate(sorted_nodes)}

# ----------------- WRITE NODES FILE -----------------

# Nodes CSV header: id,label,type,is_emotion,frequency
with open(NODES_OUT, "w", encoding="utf-8") as f:
    f.write("id,label,type,is_emotion,frequency\n")
    for label, freq in sorted_nodes:
        node_id = label_to_id[label]
        node_type = "emotion" if label in EMOTION_SET else "collocate"
        is_emotion = 1 if label in EMOTION_SET else 0
        f.write(f"{node_id},{label},{node_type},{is_emotion},{freq}\n")

print(f"Nodes file written to: {NODES_OUT}")

# ----------------- WRITE EDGES FILE -----------------

# Edges CSV header: source,target,weight,type
with open(EDGES_OUT, "w", encoding="utf-8") as f:
    f.write("source,target,weight,type\n")
    for (label_a, label_b), weight in edge_weights.items():
        # Ensure both nodes exist
        if label_a not in label_to_id or label_b not in label_to_id:
            continue
        source_id = label_to_id[label_a]
        target_id = label_to_id[label_b]
        f.write(f"{source_id},{target_id},{weight},undirected\n")

print(f"Edges file written to: {EDGES_OUT}")
print("Done.")
